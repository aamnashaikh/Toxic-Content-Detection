{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJFHDGgvXXjG",
        "outputId": "659d48db-14eb-41fb-c08c-4809a06e2b28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/608.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m604.2/608.4 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install transformers datasets torch scikit-learn pandas matplotlib seaborn nltk emoji tensorflow openpyxl --quiet\n",
        "\n",
        "# Check if running in Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"✓ Running in Google Colab\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"✓ Running locally\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8WB3VfSgj-V"
      },
      "source": [
        " **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dpfPC8Hghwi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re, nltk, emoji\n",
        "import torch\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,confusion_matrix,classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\n",
        "\n",
        "# Check if running in Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"✓ Running in Google Colab\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"✓ Running locally\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Option: Upload Files Directly (Google Colab)**\n",
        "\n",
        "If you want to upload files directly in Colab, uncomment and run the cell below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment the lines below if you want to upload files directly in Colab\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# # After uploading, files will be in the current directory\n",
        "# # You can then proceed to the next cell\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OlRn0hkgs1q"
      },
      "source": [
        "**Load Dataset Files**\n",
        "\n",
        "**For Google Colab Users:**\n",
        "1. Upload both dataset files using the file uploader below, OR\n",
        "2. Mount Google Drive and place files in your Drive folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "a47f1BZOgrz9",
        "outputId": "ec5844d5-2dfb-45fa-ccae-534c711ed4ad"
      },
      "outputs": [
        {
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode bytes in position 15-16: invalid continuation byte",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1467260260.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Replace names with your uploaded file names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Urdu Abusive Dataset.csv\"\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_tsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Hate Speech Roman Urdu (HS-RU-20).xlsx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CSV dataset:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_csv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Fail here loudly instead of in cython after reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 15-16: invalid continuation byte"
          ]
        }
      ],
      "source": [
        "# Google Colab: File Upload Option (Uncomment if uploading files directly)\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()  # This will prompt you to upload files\n",
        "\n",
        "# Google Colab: Mount Google Drive Option (Uncomment if using Drive)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# # Then update file paths below to: '/content/drive/MyDrive/your_folder/filename.xlsx'\n",
        "\n",
        "import os\n",
        "\n",
        "# Determine file paths based on environment\n",
        "if IN_COLAB:\n",
        "    # Try multiple possible locations in Colab\n",
        "    possible_paths_csv = [\n",
        "        \"Urdu Abusive Dataset.csv\",\n",
        "        \"/content/Urdu Abusive Dataset.csv\",\n",
        "        \"/content/drive/MyDrive/Urdu Abusive Dataset.csv\",\n",
        "        \"/content/drive/My Drive/Urdu Abusive Dataset.csv\"\n",
        "    ]\n",
        "    possible_paths_xlsx = [\n",
        "        \"Hate Speech Roman Urdu (HS-RU-20).xlsx\",\n",
        "        \"/content/Hate Speech Roman Urdu (HS-RU-20).xlsx\",\n",
        "        \"/content/drive/MyDrive/Hate Speech Roman Urdu (HS-RU-20).xlsx\",\n",
        "        \"/content/drive/My Drive/Hate Speech Roman Urdu (HS-RU-20).xlsx\"\n",
        "    ]\n",
        "else:\n",
        "    # Local paths\n",
        "    possible_paths_csv = [\"Urdu Abusive Dataset.csv\"]\n",
        "    possible_paths_xlsx = [\"Hate Speech Roman Urdu (HS-RU-20).xlsx\"]\n",
        "\n",
        "# Find and load first dataset\n",
        "df_csv = pd.DataFrame()\n",
        "csv_file_path = None\n",
        "for path in possible_paths_csv:\n",
        "    if os.path.exists(path):\n",
        "        csv_file_path = path\n",
        "        break\n",
        "\n",
        "if csv_file_path:\n",
        "    try:\n",
        "        df_csv = pd.read_excel(csv_file_path, engine='openpyxl')\n",
        "        print(f\"✓ Loaded Urdu Abusive Dataset.csv from: {csv_file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error loading first file: {e}\")\n",
        "        df_csv = pd.DataFrame()\n",
        "else:\n",
        "    print(\"✗ Urdu Abusive Dataset.csv not found. Please upload it or check the path.\")\n",
        "    print(\"  Searched in:\", possible_paths_csv)\n",
        "\n",
        "# Find and load second dataset\n",
        "df_tsv = pd.DataFrame()\n",
        "xlsx_file_path = None\n",
        "for path in possible_paths_xlsx:\n",
        "    if os.path.exists(path):\n",
        "        xlsx_file_path = path\n",
        "        break\n",
        "\n",
        "if xlsx_file_path:\n",
        "    try:\n",
        "        df_tsv = pd.read_excel(xlsx_file_path, engine='openpyxl')\n",
        "        print(f\"✓ Loaded Hate Speech Roman Urdu (HS-RU-20).xlsx from: {xlsx_file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error loading second file: {e}\")\n",
        "        df_tsv = pd.DataFrame()\n",
        "else:\n",
        "    print(\"✗ Hate Speech Roman Urdu (HS-RU-20).xlsx not found. Please upload it or check the path.\")\n",
        "    print(\"  Searched in:\", possible_paths_xlsx)\n",
        "\n",
        "# Display dataset info\n",
        "if not df_csv.empty:\n",
        "    print(f\"\\n✓ First dataset shape: {df_csv.shape}\")\n",
        "    print(f\"✓ First dataset columns: {df_csv.columns.tolist()}\")\n",
        "else:\n",
        "    print(\"\\n✗ First dataset is empty\")\n",
        "\n",
        "if not df_tsv.empty:\n",
        "    print(f\"\\n✓ Second dataset shape: {df_tsv.shape}\")\n",
        "    print(f\"✓ Second dataset columns: {df_tsv.columns.tolist()}\")\n",
        "else:\n",
        "    print(\"\\n✗ Second dataset is empty\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYxDY1vnhGeT"
      },
      "source": [
        "**Standardize Columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hlxcv4rhDZ-"
      },
      "outputs": [],
      "source": [
        "# Auto-detect and standardize columns to 'text' and 'label'\n",
        "# Common text column names\n",
        "text_cols = ['comment', 'tweet', 'message', 'content', 'text', 'Comment', 'Tweet', 'Message', 'Content', 'Text',\n",
        "             'comment_text', 'Comment_Text', 'sentence', 'Sentence']\n",
        "# Common label column names\n",
        "label_cols = ['label', 'Label', 'class', 'Class', 'category', 'Category', 'toxic', 'Toxic', 'hate', 'Hate',\n",
        "              'comment_class', 'Comment_Class', 'Neutral (N) / Hostile (H)', 'neutral (n) / hostile (h)']\n",
        "\n",
        "def standardize_columns(df, dataset_name):\n",
        "    \"\"\"Auto-detect and rename columns to standard 'text' and 'label'\"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Find text column\n",
        "    text_col = None\n",
        "    for col in text_cols:\n",
        "        if col in df.columns:\n",
        "            text_col = col\n",
        "            break\n",
        "    \n",
        "    # Find label column\n",
        "    label_col = None\n",
        "    for col in label_cols:\n",
        "        if col in df.columns:\n",
        "            label_col = col\n",
        "            break\n",
        "    \n",
        "    # If not found, try to infer\n",
        "    if text_col is None:\n",
        "        # Check for columns with text-like content\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype == 'object' and col != label_col:\n",
        "                text_col = col\n",
        "                break\n",
        "    \n",
        "    if label_col is None:\n",
        "        # Check for numeric or categorical columns that might be labels\n",
        "        for col in df.columns:\n",
        "            if col != text_col and (df[col].dtype in ['int64', 'float64'] or df[col].dtype.name == 'category'):\n",
        "                label_col = col\n",
        "                break\n",
        "    \n",
        "    if text_col:\n",
        "        df = df.rename(columns={text_col: 'text'})\n",
        "        print(f\"{dataset_name}: Text column '{text_col}' -> 'text'\")\n",
        "    else:\n",
        "        print(f\"Warning: Could not find text column in {dataset_name}\")\n",
        "    \n",
        "    if label_col:\n",
        "        df = df.rename(columns={label_col: 'label'})\n",
        "        print(f\"{dataset_name}: Label column '{label_col}' -> 'label'\")\n",
        "    else:\n",
        "        print(f\"Warning: Could not find label column in {dataset_name}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "df_csv = standardize_columns(df_csv, \"Dataset 1\")\n",
        "df_tsv = standardize_columns(df_tsv, \"Dataset 2\")\n",
        "\n",
        "print(\"\\nDataset 1 columns after standardization:\", df_csv.columns.tolist())\n",
        "print(\"Dataset 2 columns after standardization:\", df_tsv.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBjWWNkvhMk5"
      },
      "source": [
        "**Merge Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4crwQos3hQhz"
      },
      "outputs": [],
      "source": [
        "# Merge datasets\n",
        "df = pd.concat([df_csv, df_tsv], axis=0, ignore_index=True)\n",
        "\n",
        "# Keep only text and label columns\n",
        "if 'text' in df.columns and 'label' in df.columns:\n",
        "    df = df[['text', 'label']]\n",
        "else:\n",
        "    print(\"Error: 'text' or 'label' columns not found after merging\")\n",
        "    print(\"Available columns:\", df.columns.tolist())\n",
        "\n",
        "# Remove duplicates and nulls\n",
        "df = df.drop_duplicates(subset=[\"text\"])\n",
        "df = df.dropna(subset=[\"text\", \"label\"])\n",
        "\n",
        "# Standardize labels to binary (0/1)\n",
        "# Handle various label formats\n",
        "def standardize_label(label):\n",
        "    \"\"\"Convert various label formats to binary 0/1\"\"\"\n",
        "    if pd.isna(label):\n",
        "        return None\n",
        "    \n",
        "    # Handle boolean values directly\n",
        "    if isinstance(label, bool):\n",
        "        return 1 if label else 0\n",
        "    \n",
        "    # Convert to string for comparison\n",
        "    label_str = str(label).lower().strip()\n",
        "    \n",
        "    # Handle numeric labels\n",
        "    if label_str in ['0', '0.0', '0.00']:\n",
        "        return 0\n",
        "    if label_str in ['1', '1.0', '1.00']:\n",
        "        return 1\n",
        "    \n",
        "    # Handle text labels (non-toxic/negative = 0, toxic/positive = 1)\n",
        "    non_toxic = ['non-toxic', 'nontoxic', 'non_toxic', 'negative', 'normal', 'clean', 'safe', 'no', 'false', '0', 'n', 'neutral']\n",
        "    toxic = ['toxic', 'hate', 'abusive', 'positive', 'yes', 'true', '1', 'h', 'hostile']\n",
        "    \n",
        "    if label_str in non_toxic:\n",
        "        return 0\n",
        "    if label_str in toxic:\n",
        "        return 1\n",
        "    \n",
        "    # If it's a number, convert directly\n",
        "    try:\n",
        "        num = float(label_str)\n",
        "        return int(num > 0.5)  # Threshold at 0.5\n",
        "    except:\n",
        "        # Default: assume non-toxic if unclear\n",
        "        return 0\n",
        "\n",
        "df['label'] = df['label'].apply(standardize_label)\n",
        "df = df.dropna(subset=[\"label\"])  # Remove any that couldn't be standardized\n",
        "\n",
        "# Shuffle\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"Merged dataset size:\", df.shape)\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(df['label'].value_counts().sort_index())\n",
        "print(f\"\\nLabel value counts:\\n{df['label'].value_counts()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TStOv3fMhTsm"
      },
      "source": [
        "**Roman Urdu Normalization & Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DsTXhFhhauZ"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords', quiet=True)\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Extended Roman Urdu stopwords\n",
        "stopwords_roman = [\"hai\", \"hay\", \"he\", \"hain\", \"kya\", \"ha\", \"me\", \"tum\", \"nai\", \"nahi\", \"na\", \n",
        "                   \"mein\", \"main\", \"acha\", \"accha\", \"bohat\", \"bahut\", \"nh\", \"h\", \"ho\", \"hoon\",\n",
        "                   \"ka\", \"ki\", \"ke\", \"ko\", \"se\", \"par\", \"aur\", \"ya\", \"bhi\", \"to\", \"tu\"]\n",
        "\n",
        "try:\n",
        "    stopwords_urdu = set(stopwords.words('urdu'))\n",
        "except:\n",
        "    stopwords_urdu = set()\n",
        "\n",
        "# Comprehensive slang and spelling variation mapping for Roman Urdu\n",
        "slang_map = {\n",
        "    # Common spelling variations\n",
        "    \"yar\": \"yaar\", \"yarr\": \"yaar\", \"yaaar\": \"yaar\",\n",
        "    \"bhai\": \"bhai\", \"bhaii\": \"bhai\", \"bro\": \"bhai\",\n",
        "    \"ganda\": \"gandi\", \"gandha\": \"gandi\",\n",
        "    \"lanat\": \"laanat\", \"lanath\": \"laanat\",\n",
        "    \"chutiya\": \"chutiya\", \"chutia\": \"chutiya\", \"chootiya\": \"chutiya\",\n",
        "    \"bkwas\": \"bakwas\", \"bakwaas\": \"bakwas\", \"bakwass\": \"bakwas\",\n",
        "    \"larki\": \"ladki\", \"larkee\": \"ladki\", \"ladkee\": \"ladki\",\n",
        "    \"larka\": \"ladka\", \"larkaa\": \"ladka\", \"ladkaa\": \"ladka\",\n",
        "    \"tum\": \"tm\", \"tu\": \"tm\", \"tumhe\": \"tmhe\",\n",
        "    \"mein\": \"main\", \"me\": \"main\", \"mujhe\": \"mujhe\",\n",
        "    \"hai\": \"hai\", \"hay\": \"hai\", \"he\": \"hai\",\n",
        "    \"nahi\": \"nahi\", \"nai\": \"nahi\", \"na\": \"nahi\",\n",
        "    \"acha\": \"accha\", \"accha\": \"accha\", \"achha\": \"accha\",\n",
        "    \"bohat\": \"bahut\", \"bahut\": \"bahut\", \"bohot\": \"bahut\",\n",
        "    # More variations\n",
        "    \"kya\": \"kya\", \"kyaa\": \"kya\",\n",
        "    \"kar\": \"kar\", \"karr\": \"kar\",\n",
        "    \"de\": \"de\", \"dey\": \"de\",\n",
        "    \"le\": \"le\", \"ley\": \"le\",\n",
        "    \"ja\": \"ja\", \"jaa\": \"ja\",\n",
        "    \"aa\": \"aa\", \"aao\": \"aao\",\n",
        "    \"gaya\": \"gaya\", \"gaya\": \"gaya\",\n",
        "    \"aya\": \"aya\", \"aaya\": \"aya\",\n",
        "}\n",
        "\n",
        "def normalize_roman_urdu(text):\n",
        "    \"\"\"Normalize Roman Urdu spelling variations\"\"\"\n",
        "    words = text.split()\n",
        "    normalized_words = []\n",
        "    for word in words:\n",
        "        # Check exact match first\n",
        "        if word in slang_map:\n",
        "            normalized_words.append(slang_map[word])\n",
        "        else:\n",
        "            # Check case-insensitive match\n",
        "            word_lower = word.lower()\n",
        "            if word_lower in slang_map:\n",
        "                normalized_words.append(slang_map[word_lower])\n",
        "            else:\n",
        "                normalized_words.append(word)\n",
        "    return \" \".join(normalized_words)\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Comprehensive text cleaning and normalization\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    \n",
        "    text = str(text).lower()\n",
        "    \n",
        "    # Remove emojis\n",
        "    text = emoji.replace_emoji(text, \"\")\n",
        "    \n",
        "    # Remove URLs\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    \n",
        "    # Keep Urdu characters (ء-ی), English letters, numbers, and spaces\n",
        "    text = re.sub(r\"[^a-zA-Z0-9ء-ی ]\", \" \", text)\n",
        "    \n",
        "    # Normalize Roman Urdu spelling variations\n",
        "    text = normalize_roman_urdu(text)\n",
        "    \n",
        "    # Remove stopwords\n",
        "    words = text.split()\n",
        "    filtered_words = [w for w in words if w not in stopwords_roman and w not in stopwords_urdu and len(w) > 1]\n",
        "    \n",
        "    text = \" \".join(filtered_words)\n",
        "    \n",
        "    # Final cleanup\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Apply cleaning\n",
        "print(\"Cleaning and normalizing text...\")\n",
        "df['text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Remove empty texts after cleaning\n",
        "df = df[df['text'].str.len() > 0]\n",
        "\n",
        "print(f\"Dataset size after cleaning: {df.shape}\")\n",
        "print(\"\\nSample cleaned texts:\")\n",
        "print(df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAvPK5qdhd76"
      },
      "source": [
        "**Train-Test Split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvQBRK6ChjVE"
      },
      "outputs": [],
      "source": [
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df[\"text\"], \n",
        "    df[\"label\"], \n",
        "    test_size=0.2, \n",
        "    random_state=42,\n",
        "    stratify=df[\"label\"]  # Ensure balanced split\n",
        ")\n",
        "\n",
        "# Ensure labels are integers\n",
        "y_train = y_train.astype(int)\n",
        "y_test = y_test.astype(int)\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n",
        "print(f\"\\nTraining label distribution:\\n{y_train.value_counts().sort_index()}\")\n",
        "print(f\"\\nTest label distribution:\\n{y_test.value_counts().sort_index()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtEKHMkih4IZ"
      },
      "source": [
        "# **PART 1 – ML Baselines**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsa-0WFOiC65"
      },
      "source": [
        "**TF-IDF Vectorization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hi7FXAYiHsx"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul0z36P8iJ84"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVbJDkneiQbv"
      },
      "outputs": [],
      "source": [
        "# Ensure labels are numeric\n",
        "y_train_lr = y_train.astype(int)\n",
        "y_test_lr = y_test.astype(int)\n",
        "\n",
        "lr = LogisticRegression(max_iter=500, random_state=42)\n",
        "lr.fit(X_train_vec, y_train_lr)\n",
        "lr_pred = lr.predict(X_test_vec)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Logistic Regression Results\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy:  {accuracy_score(y_test_lr, lr_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_lr, lr_pred, average='weighted', zero_division=0):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_test_lr, lr_pred, average='weighted', zero_division=0):.4f}\")\n",
        "print(f\"F1-Score:  {f1_score(y_test_lr, lr_pred, average='weighted', zero_division=0):.4f}\")\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test_lr, lr_pred, zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryi-SZhNiVA3"
      },
      "source": [
        "**SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whXh7jwhiYpf"
      },
      "outputs": [],
      "source": [
        "# Ensure labels are numeric\n",
        "y_train_svm = y_train.astype(int)\n",
        "y_test_svm = y_test.astype(int)\n",
        "\n",
        "svm = SVC(kernel='linear', random_state=42)\n",
        "svm.fit(X_train_vec, y_train_svm)\n",
        "svm_pred = svm.predict(X_test_vec)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SVM Results\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy:  {accuracy_score(y_test_svm, svm_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_svm, svm_pred, average='weighted', zero_division=0):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_test_svm, svm_pred, average='weighted', zero_division=0):.4f}\")\n",
        "print(f\"F1-Score:  {f1_score(y_test_svm, svm_pred, average='weighted', zero_division=0):.4f}\")\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test_svm, svm_pred, zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9ESAgjgicGP"
      },
      "source": [
        "# **PART 2 – LSTM Deep Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYqkECIEihsw"
      },
      "source": [
        "**Prepare Tokenizer & Sequences**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQCLOejGinGP"
      },
      "outputs": [],
      "source": [
        "keras_tokenizer = Tokenizer(num_words=8000)\n",
        "keras_tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = keras_tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = keras_tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "max_len = 50\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCh4V3Xiipa3"
      },
      "source": [
        "**Build & Train LSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMMA7Jr2is32"
      },
      "outputs": [],
      "source": [
        "# Ensure labels are numeric and properly formatted for LSTM\n",
        "y_train_lstm = y_train.astype(int).values\n",
        "y_test_lstm = y_test.astype(int).values\n",
        "\n",
        "model_lstm = Sequential([\n",
        "    Embedding(input_dim=8000, output_dim=128, input_length=max_len),\n",
        "    LSTM(64, return_sequences=False),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training LSTM model...\")\n",
        "history = model_lstm.fit(\n",
        "    X_train_pad, y_train_lstm, \n",
        "    validation_split=0.2, \n",
        "    epochs=4, \n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "lstm_pred_proba = model_lstm.predict(X_test_pad, verbose=0)\n",
        "lstm_pred = (lstm_pred_proba > 0.5).astype(int).flatten()\n",
        "\n",
        "# Ensure predictions are same length as test labels\n",
        "if len(lstm_pred) != len(y_test_lstm):\n",
        "    print(f\"Warning: Prediction length {len(lstm_pred)} != test length {len(y_test_lstm)}\")\n",
        "    min_len = min(len(lstm_pred), len(y_test_lstm))\n",
        "    lstm_pred = lstm_pred[:min_len]\n",
        "    y_test_lstm = y_test_lstm[:min_len]\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"LSTM Results\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy:  {accuracy_score(y_test_lstm, lstm_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_lstm, lstm_pred, average='weighted', zero_division=0):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_test_lstm, lstm_pred, average='weighted', zero_division=0):.4f}\")\n",
        "print(f\"F1-Score:  {f1_score(y_test_lstm, lstm_pred, average='weighted', zero_division=0):.4f}\")\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test_lstm, lstm_pred, zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPRXQ2GkivN-"
      },
      "source": [
        "# **PART 3 – Transformer Model (XLM-RoBERTa)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm-wjL5Jiz6r"
      },
      "source": [
        "**Tokenizer & Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BJ19hsqi4Gf"
      },
      "outputs": [],
      "source": [
        "model_name = \"cardiffnlp/twitter-xlm-roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WQvMX8Ni58G"
      },
      "source": [
        "**Tokenize Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91Q-kyi3jAX9"
      },
      "outputs": [],
      "source": [
        "# Ensure labels are numeric\n",
        "y_train_xlm = y_train.astype(int).values\n",
        "y_test_xlm = y_test.astype(int).values\n",
        "\n",
        "# Tokenize data\n",
        "train_enc = tokenizer(list(X_train), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "test_enc = tokenizer(list(X_test), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "class HSDDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "train_dataset_xlm = HSDDataset(train_enc, y_train_xlm)\n",
        "test_dataset_xlm = HSDDataset(test_enc, y_test_xlm)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset_xlm)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset_xlm)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K1morayjEIG"
      },
      "source": [
        "**Train Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IoqiHcVjIH1"
      },
      "outputs": [],
      "source": [
        "# Set output directory based on environment\n",
        "if IN_COLAB:\n",
        "    output_dir_xlm = \"/content/results_xlm\"\n",
        "else:\n",
        "    output_dir_xlm = \"./results_xlm\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir_xlm,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    greater_is_better=True,\n",
        "    seed=42,\n",
        "    fp16=False  # Set to True if using GPU in Colab\n",
        ")\n",
        "\n",
        "# Define compute_metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\n",
        "        'accuracy': accuracy_score(labels, predictions),\n",
        "        'precision': precision_score(labels, predictions, average='weighted', zero_division=0),\n",
        "        'recall': recall_score(labels, predictions, average='weighted', zero_division=0),\n",
        "        'f1': f1_score(labels, predictions, average='weighted', zero_division=0)\n",
        "    }\n",
        "\n",
        "trainer_xlm = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset_xlm,\n",
        "    eval_dataset=test_dataset_xlm,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"Training XLM-RoBERTa model...\")\n",
        "trainer_xlm.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NncDN5CIjK9W"
      },
      "source": [
        "**Evaluate Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlSeRnJ1jQDN"
      },
      "outputs": [],
      "source": [
        "# Evaluate XLM-RoBERTa\n",
        "print(\"\\nEvaluating XLM-RoBERTa...\")\n",
        "try:\n",
        "    predictions_xlm = trainer_xlm.predict(test_dataset_xlm)\n",
        "    preds_xlm = predictions_xlm.predictions.argmax(axis=1)\n",
        "except NameError:\n",
        "    raise NameError(\"trainer_xlm is not defined. Please run the training cell first.\")\n",
        "\n",
        "# Ensure same length\n",
        "if len(preds_xlm) != len(y_test_xlm):\n",
        "    min_len = min(len(preds_xlm), len(y_test_xlm))\n",
        "    preds_xlm = preds_xlm[:min_len]\n",
        "    y_test_xlm = y_test_xlm[:min_len]\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"XLM-RoBERTa Results\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy:  {accuracy_score(y_test_xlm, preds_xlm):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_xlm, preds_xlm, average='weighted', zero_division=0):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_test_xlm, preds_xlm, average='weighted', zero_division=0):.4f}\")\n",
        "print(f\"F1-Score:  {f1_score(y_test_xlm, preds_xlm, average='weighted', zero_division=0):.4f}\")\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test_xlm, preds_xlm, zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **PART 4 – Transformer Model (mBERT)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tokenizer & Model (mBERT)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load mBERT model\n",
        "model_name_mbert = \"bert-base-multilingual-cased\"\n",
        "tokenizer_mbert = AutoTokenizer.from_pretrained(model_name_mbert)\n",
        "model_mbert = AutoModelForSequenceClassification.from_pretrained(model_name_mbert, num_labels=2)\n",
        "\n",
        "print(f\"Loaded mBERT model: {model_name_mbert}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tokenize Data (mBERT)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure labels are numeric\n",
        "y_train_mbert = y_train.astype(int).values\n",
        "y_test_mbert = y_test.astype(int).values\n",
        "\n",
        "# Tokenize data for mBERT\n",
        "train_enc_mbert = tokenizer_mbert(list(X_train), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "test_enc_mbert = tokenizer_mbert(list(X_test), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "train_dataset_mbert = HSDDataset(train_enc_mbert, y_train_mbert)\n",
        "test_dataset_mbert = HSDDataset(test_enc_mbert, y_test_mbert)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset_mbert)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset_mbert)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Train Transformer (mBERT)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set output directory based on environment\n",
        "if IN_COLAB:\n",
        "    output_dir_mbert = \"/content/results_mbert\"\n",
        "else:\n",
        "    output_dir_mbert = \"./results_mbert\"\n",
        "\n",
        "training_args_mbert = TrainingArguments(\n",
        "    output_dir=output_dir_mbert,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    greater_is_better=True,\n",
        "    seed=42,\n",
        "    fp16=False  # Set to True if using GPU in Colab\n",
        ")\n",
        "\n",
        "trainer_mbert = Trainer(\n",
        "    model=model_mbert,\n",
        "    args=training_args_mbert,\n",
        "    train_dataset=train_dataset_mbert,\n",
        "    eval_dataset=test_dataset_mbert,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"Training mBERT model...\")\n",
        "trainer_mbert.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Evaluate Transformer (mBERT)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate mBERT\n",
        "print(\"\\nEvaluating mBERT...\")\n",
        "try:\n",
        "    predictions_mbert = trainer_mbert.predict(test_dataset_mbert)\n",
        "    preds_mbert = predictions_mbert.predictions.argmax(axis=1)\n",
        "except NameError:\n",
        "    raise NameError(\"trainer_mbert is not defined. Please run the training cell first.\")\n",
        "\n",
        "# Ensure same length\n",
        "if len(preds_mbert) != len(y_test_mbert):\n",
        "    min_len = min(len(preds_mbert), len(y_test_mbert))\n",
        "    preds_mbert = preds_mbert[:min_len]\n",
        "    y_test_mbert = y_test_mbert[:min_len]\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"mBERT Results\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy:  {accuracy_score(y_test_mbert, preds_mbert):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_mbert, preds_mbert, average='weighted', zero_division=0):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_test_mbert, preds_mbert, average='weighted', zero_division=0):.4f}\")\n",
        "print(f\"F1-Score:  {f1_score(y_test_mbert, preds_mbert, average='weighted', zero_division=0):.4f}\")\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test_mbert, preds_mbert, zero_division=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7VK32iVjPm1"
      },
      "source": [
        "**Confusion Matrices for All Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLD3Yqi0jXZk"
      },
      "outputs": [],
      "source": [
        "# Prepare predictions for all models (ensure consistent test labels)\n",
        "# Use the same test labels for all models - get the minimum length to ensure consistency\n",
        "y_test_final = y_test.astype(int).values\n",
        "\n",
        "# Find minimum length across all available test sets\n",
        "test_lengths = [len(y_test_final)]\n",
        "try:\n",
        "    test_lengths.append(len(y_test_lr))\n",
        "except NameError:\n",
        "    pass\n",
        "try:\n",
        "    test_lengths.append(len(y_test_svm))\n",
        "except NameError:\n",
        "    pass\n",
        "try:\n",
        "    test_lengths.append(len(y_test_lstm))\n",
        "except NameError:\n",
        "    pass\n",
        "try:\n",
        "    test_lengths.append(len(y_test_xlm))\n",
        "except NameError:\n",
        "    pass\n",
        "try:\n",
        "    test_lengths.append(len(y_test_mbert))\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "min_test_len = min(test_lengths)\n",
        "print(f\"Using minimum test length: {min_test_len} for consistent evaluation\")\n",
        "\n",
        "# Align all predictions and labels to same length\n",
        "def align_predictions(pred, target_len):\n",
        "    \"\"\"Align predictions to target length\"\"\"\n",
        "    if len(pred) != target_len:\n",
        "        if len(pred) > target_len:\n",
        "            return pred[:target_len]\n",
        "        else:\n",
        "            # Pad with zeros if shorter (shouldn't happen, but safety check)\n",
        "            return np.pad(pred, (0, target_len - len(pred)), mode='constant')\n",
        "    return pred\n",
        "\n",
        "# Align all predictions and labels (only if they exist)\n",
        "try:\n",
        "    lr_pred_aligned = align_predictions(lr_pred, min_test_len)\n",
        "    y_test_lr_aligned = y_test_lr[:min_test_len]\n",
        "except NameError:\n",
        "    lr_pred_aligned = None\n",
        "    y_test_lr_aligned = None\n",
        "\n",
        "try:\n",
        "    svm_pred_aligned = align_predictions(svm_pred, min_test_len)\n",
        "    y_test_svm_aligned = y_test_svm[:min_test_len]\n",
        "except NameError:\n",
        "    svm_pred_aligned = None\n",
        "    y_test_svm_aligned = None\n",
        "\n",
        "try:\n",
        "    lstm_pred_aligned = align_predictions(lstm_pred, min_test_len)\n",
        "    y_test_lstm_aligned = y_test_lstm[:min_test_len]\n",
        "except NameError:\n",
        "    lstm_pred_aligned = None\n",
        "    y_test_lstm_aligned = None\n",
        "\n",
        "try:\n",
        "    preds_xlm_aligned = align_predictions(preds_xlm, min_test_len)\n",
        "    y_test_xlm_aligned = y_test_xlm[:min_test_len]\n",
        "except NameError:\n",
        "    preds_xlm_aligned = None\n",
        "    y_test_xlm_aligned = None\n",
        "\n",
        "try:\n",
        "    preds_mbert_aligned = align_predictions(preds_mbert, min_test_len)\n",
        "    y_test_mbert_aligned = y_test_mbert[:min_test_len]\n",
        "except NameError:\n",
        "    preds_mbert_aligned = None\n",
        "    y_test_mbert_aligned = None\n",
        "\n",
        "y_test_final_aligned = y_test_final[:min_test_len]\n",
        "\n",
        "# Build model_preds dictionary only for available models\n",
        "model_preds = {}\n",
        "if lr_pred_aligned is not None and y_test_lr_aligned is not None:\n",
        "    model_preds[\"Logistic Regression\"] = (lr_pred_aligned, y_test_lr_aligned)\n",
        "if svm_pred_aligned is not None and y_test_svm_aligned is not None:\n",
        "    model_preds[\"SVM\"] = (svm_pred_aligned, y_test_svm_aligned)\n",
        "if lstm_pred_aligned is not None and y_test_lstm_aligned is not None:\n",
        "    model_preds[\"LSTM\"] = (lstm_pred_aligned, y_test_lstm_aligned)\n",
        "if preds_xlm_aligned is not None and y_test_xlm_aligned is not None:\n",
        "    model_preds[\"XLM-RoBERTa\"] = (preds_xlm_aligned, y_test_xlm_aligned)\n",
        "if preds_mbert_aligned is not None and y_test_mbert_aligned is not None:\n",
        "    model_preds[\"mBERT\"] = (preds_mbert_aligned, y_test_mbert_aligned)\n",
        "\n",
        "# Plot confusion matrices\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, (name, (pred, y_true)) in enumerate(model_preds.items()):\n",
        "    # Ensure same length\n",
        "    min_len = min(len(pred), len(y_true))\n",
        "    pred_trimmed = pred[:min_len]\n",
        "    y_true_trimmed = y_true[:min_len]\n",
        "    \n",
        "    cm = confusion_matrix(y_true_trimmed, pred_trimmed)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[idx])\n",
        "    axes[idx].set_title(f\"{name} Confusion Matrix\", fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_xlabel(\"Predicted\")\n",
        "    axes[idx].set_ylabel(\"Actual\")\n",
        "\n",
        "# Hide the last subplot if odd number of models\n",
        "if len(model_preds) < len(axes):\n",
        "    axes[-1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create comprehensive comparison table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results_summary = []\n",
        "\n",
        "for name, (pred, y_true) in model_preds.items():\n",
        "    min_len = min(len(pred), len(y_true))\n",
        "    pred_trimmed = pred[:min_len]\n",
        "    y_true_trimmed = y_true[:min_len]\n",
        "    \n",
        "    acc = accuracy_score(y_true_trimmed, pred_trimmed)\n",
        "    prec = precision_score(y_true_trimmed, pred_trimmed, average='weighted', zero_division=0)\n",
        "    rec = recall_score(y_true_trimmed, pred_trimmed, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_true_trimmed, pred_trimmed, average='weighted', zero_division=0)\n",
        "    \n",
        "    results_summary.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': f\"{acc:.4f}\",\n",
        "        'Precision': f\"{prec:.4f}\",\n",
        "        'Recall': f\"{rec:.4f}\",\n",
        "        'F1-Score': f\"{f1:.4f}\"\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results_summary)\n",
        "print(\"\\n\" + results_df.to_string(index=False))\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
